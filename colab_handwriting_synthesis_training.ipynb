{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwriting Synthesis Model Training on Google Colab\n",
    "\n",
    "This notebook trains a handwriting synthesis model using the `finnbusse/v3testing` dataset from Hugging Face.\n",
    "\n",
    "## Features:\n",
    "- Loads stroke data from HuggingFace Parquet format\n",
    "- Optimized hyperparameters for different dataset sizes (565, 750, 1000, 1250, 1500, 1750, 2000 words)\n",
    "- tqdm progress bars for training visualization\n",
    "- Sample generation every 10 epochs\n",
    "- Training graphs and model saving at the end\n",
    "\n",
    "**Note:** Make sure to select a GPU runtime: Runtime > Change runtime type > GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/finnbusse/pytorch-handwriting-synthesis-toolkit.git\n",
    "%cd pytorch-handwriting-synthesis-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (compatible with Google Colab Python 3.11+ and PyTorch 2.x)\n",
    "# Note: Colab already has torch, numpy, matplotlib pre-installed\n",
    "!pip install -q Pillow h5py svgwrite datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, Image, clear_output\n",
    "from datasets import load_dataset\n",
    "import h5py\n",
    "\n",
    "# Add the repository to the path\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from handwriting_synthesis import data, models, utils, losses, metrics\n",
    "from handwriting_synthesis.data import Tokenizer\n",
    "from handwriting_synthesis.sampling import HandwritingSynthesizer\n",
    "from handwriting_synthesis.tasks import HandwritingSynthesisTask\n",
    "from handwriting_synthesis.optimizers import CustomRMSprop\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Select your dataset size and corresponding optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Optimized hyperparameters for different dataset sizes\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset settings\n",
    "HUGGINGFACE_DATASET = \"finnbusse/v3testing\"\n",
    "\n",
    "# Select your dataset size (approximate number of words)\n",
    "# Options: 565, 750, 1000, 1250, 1500, 1750, 2000\n",
    "DATASET_SIZE = 565  # Change this to match your dataset size\n",
    "\n",
    "# Optimized hyperparameters for each dataset size\n",
    "# These are carefully tuned for handwriting synthesis on small-to-medium datasets\n",
    "HYPERPARAMETERS = {\n",
    "    565: {\n",
    "        'batch_size': 16,          # Larger batch for faster training on GPU\n",
    "        'epochs': 200,             # More epochs for small dataset\n",
    "        'learning_rate': 0.0001,   # Standard LR\n",
    "        'hidden_size': 400,        # Standard hidden size\n",
    "        'gradient_clip_output': 100,\n",
    "        'gradient_clip_lstm': 10,\n",
    "        'validation_split': 0.15,  # 15% for validation\n",
    "        'max_seq_length': 500,     # Max points per stroke sequence\n",
    "        'sample_interval': 5,      # Show sample every N epochs\n",
    "    },\n",
    "    750: {\n",
    "        'batch_size': 20,\n",
    "        'epochs': 180,\n",
    "        'learning_rate': 0.0001,\n",
    "        'hidden_size': 400,\n",
    "        'gradient_clip_output': 100,\n",
    "        'gradient_clip_lstm': 10,\n",
    "        'validation_split': 0.15,\n",
    "        'max_seq_length': 500,\n",
    "        'sample_interval': 5,\n",
    "    },\n",
    "    1000: {\n",
    "        'batch_size': 24,\n",
    "        'epochs': 150,\n",
    "        'learning_rate': 0.0001,\n",
    "        'hidden_size': 400,\n",
    "        'gradient_clip_output': 100,\n",
    "        'gradient_clip_lstm': 10,\n",
    "        'validation_split': 0.12,\n",
    "        'max_seq_length': 500,\n",
    "        'sample_interval': 5,\n",
    "    },\n",
    "    1250: {\n",
    "        'batch_size': 28,\n",
    "        'epochs': 130,\n",
    "        'learning_rate': 0.0001,\n",
    "        'hidden_size': 400,\n",
    "        'gradient_clip_output': 100,\n",
    "        'gradient_clip_lstm': 10,\n",
    "        'validation_split': 0.12,\n",
    "        'max_seq_length': 500,\n",
    "        'sample_interval': 5,\n",
    "    },\n",
    "    1500: {\n",
    "        'batch_size': 32,\n",
    "        'epochs': 120,\n",
    "        'learning_rate': 0.0001,\n",
    "        'hidden_size': 400,\n",
    "        'gradient_clip_output': 100,\n",
    "        'gradient_clip_lstm': 10,\n",
    "        'validation_split': 0.10,\n",
    "        'max_seq_length': 500,\n",
    "        'sample_interval': 5,\n",
    "    },\n",
    "    1750: {\n",
    "        'batch_size': 32,\n",
    "        'epochs': 110,\n",
    "        'learning_rate': 0.0001,\n",
    "        'hidden_size': 400,\n",
    "        'gradient_clip_output': 100,\n",
    "        'gradient_clip_lstm': 10,\n",
    "        'validation_split': 0.10,\n",
    "        'max_seq_length': 500,\n",
    "        'sample_interval': 5,\n",
    "    },\n",
    "    2000: {\n",
    "        'batch_size': 32,\n",
    "        'epochs': 100,\n",
    "        'learning_rate': 0.0001,\n",
    "        'hidden_size': 400,\n",
    "        'gradient_clip_output': 100,\n",
    "        'gradient_clip_lstm': 10,\n",
    "        'validation_split': 0.10,\n",
    "        'max_seq_length': 500,\n",
    "        'sample_interval': 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Get hyperparameters for selected dataset size (use closest match)\n",
    "available_sizes = sorted(HYPERPARAMETERS.keys())\n",
    "selected_size = min(available_sizes, key=lambda x: abs(x - DATASET_SIZE))\n",
    "config = HYPERPARAMETERS[selected_size]\n",
    "\n",
    "print(f\"Selected configuration for ~{selected_size} words:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from HuggingFace\n",
    "print(f\"Loading dataset: {HUGGINGFACE_DATASET}\")\n",
    "try:\n",
    "    dataset = load_dataset(HUGGINGFACE_DATASET)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"\\nPossible causes:\")\n",
    "    print(\"  - Dataset not found or requires authentication\")\n",
    "    print(\"  - Network connectivity issues\")\n",
    "    print(\"  - Invalid dataset name\")\n",
    "    print(\"\\nPlease verify the dataset name and try again.\")\n",
    "    raise\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Get the training split (or the main split if no train split exists)\n",
    "if 'train' in dataset:\n",
    "    raw_data = dataset['train']\n",
    "else:\n",
    "    # Use the first available split\n",
    "    split_name = list(dataset.keys())[0]\n",
    "    raw_data = dataset[split_name]\n",
    "\n",
    "print(f\"\\nTotal samples: {len(raw_data)}\")\n",
    "print(f\"\\nSample data structure (first item):\")\n",
    "sample = raw_data[0]\n",
    "for key in sample.keys():\n",
    "    value = sample[key]\n",
    "    if isinstance(value, (list, dict)):\n",
    "        print(f\"  {key}: {type(value).__name__} (length: {len(value) if hasattr(value, '__len__') else 'N/A'})\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Conversion and Preprocessing\n",
    "\n",
    "The dataset already contains preprocessed delta/offset data (`dx`, `dy`, `eos`), which is the format needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(raw_data, max_seq_length=500):\n",
    "    \"\"\"\n",
    "    Prepare training data from HuggingFace dataset.\n",
    "    \n",
    "    The finnbusse/v3testing dataset format:\n",
    "      - dx: list of x deltas (offsets)\n",
    "      - dy: list of y deltas (offsets)\n",
    "      - eos: list of end-of-stroke flags (0 or 1)\n",
    "      - scale: scale factor used during preprocessing\n",
    "      - text: the word that was written\n",
    "    \n",
    "    This is already in the offset format expected by the training pipeline!\n",
    "    We just need to combine dx, dy, eos into (dx, dy, eos) tuples.\n",
    "    \n",
    "    Returns:\n",
    "        List of (offsets, text) tuples where offsets is a list of (dx, dy, eos) tuples\n",
    "    \"\"\"\n",
    "    prepared_data = []\n",
    "    skipped_empty = 0\n",
    "    skipped_long = 0\n",
    "    \n",
    "    for item in tqdm(raw_data, desc=\"Converting data\"):\n",
    "        text = item.get('text', '')\n",
    "        dx = item.get('dx', [])\n",
    "        dy = item.get('dy', [])\n",
    "        eos = item.get('eos', [])\n",
    "        \n",
    "        # Skip if no text or no stroke data\n",
    "        if not text or not dx or not dy or not eos:\n",
    "            skipped_empty += 1\n",
    "            continue\n",
    "        \n",
    "        # Ensure all lists have same length\n",
    "        min_len = min(len(dx), len(dy), len(eos))\n",
    "        if min_len == 0:\n",
    "            skipped_empty += 1\n",
    "            continue\n",
    "        \n",
    "        # Check sequence length\n",
    "        if min_len > max_seq_length:\n",
    "            skipped_long += 1\n",
    "            continue\n",
    "        \n",
    "        # Combine into offset tuples (dx, dy, eos)\n",
    "        offsets = [(float(dx[i]), float(dy[i]), int(eos[i])) for i in range(min_len)]\n",
    "        \n",
    "        # Make sure last point has eos=1\n",
    "        if offsets:\n",
    "            last = offsets[-1]\n",
    "            offsets[-1] = (last[0], last[1], 1)\n",
    "        \n",
    "        prepared_data.append((offsets, text))\n",
    "    \n",
    "    if skipped_empty > 0:\n",
    "        print(f\"Skipped {skipped_empty} samples with empty data\")\n",
    "    if skipped_long > 0:\n",
    "        print(f\"Skipped {skipped_long} samples exceeding max length {max_seq_length}\")\n",
    "    \n",
    "    return prepared_data\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "print(\"Preparing training data...\")\n",
    "all_data = prepare_training_data(raw_data, config['max_seq_length'])\n",
    "print(f\"\\nPrepared {len(all_data)} valid samples\")\n",
    "\n",
    "# Show a sample\n",
    "if all_data:\n",
    "    sample_offsets, sample_text = all_data[0]\n",
    "    print(f\"\\nSample:\")\n",
    "    print(f\"  Text: '{sample_text}'\")\n",
    "    print(f\"  Sequence length: {len(sample_offsets)}\")\n",
    "    print(f\"  First 5 offsets: {sample_offsets[:5]}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No valid samples found! Check dataset format.\")\n",
    "    print(f\"Dataset columns: {raw_data.column_names}\")\n",
    "    if len(raw_data) > 0:\n",
    "        print(f\"First item keys: {list(raw_data[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled_data = all_data.copy()\n",
    "random.shuffle(shuffled_data)\n",
    "\n",
    "# Split\n",
    "val_size = int(len(shuffled_data) * config['validation_split'])\n",
    "train_size = len(shuffled_data) - val_size\n",
    "\n",
    "train_data = shuffled_data[:train_size]\n",
    "val_data = shuffled_data[train_size:]\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Character Set (Charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build charset from training data\n",
    "def build_charset(data):\n",
    "    \"\"\"Build character set from the text data.\"\"\"\n",
    "    charset = set()\n",
    "    for _, text in data:\n",
    "        charset.update(set(text))\n",
    "    # Sort for consistency\n",
    "    return ''.join(sorted(charset))\n",
    "\n",
    "charset = build_charset(train_data)\n",
    "print(f\"Charset ({len(charset)} characters): '{charset}'\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(charset)\n",
    "print(f\"Tokenizer size (with padding): {tokenizer.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create H5 Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "DATA_DIR = 'colab_data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def save_offsets_to_h5(data_list, save_path, max_length):\n",
    "    \"\"\"\n",
    "    Save offset data to H5 format.\n",
    "    \n",
    "    The data is already in offset format (dx, dy, eos), so we just need to:\n",
    "    1. Truncate if necessary\n",
    "    2. Save to H5 with computed mean and std\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for offsets, text in tqdm(data_list, desc=f\"Processing {os.path.basename(save_path)}\"):\n",
    "        # Truncate if needed\n",
    "        truncated = data.truncate_sequence(offsets, max_length)\n",
    "        \n",
    "        if truncated:\n",
    "            processed.append((truncated, text))\n",
    "    \n",
    "    # Save to H5\n",
    "    data.save_to_h5(processed, save_path, max_length)\n",
    "    \n",
    "    return len(processed)\n",
    "\n",
    "\n",
    "# Determine max sequence length from data\n",
    "max_lengths = [len(offsets) for offsets, _ in train_data]\n",
    "\n",
    "actual_max_length = max(max_lengths) if max_lengths else config['max_seq_length']\n",
    "max_length = min(actual_max_length, config['max_seq_length'])\n",
    "print(f\"Maximum sequence length in data: {actual_max_length}\")\n",
    "print(f\"Using max length: {max_length}\")\n",
    "\n",
    "# Process and save training data\n",
    "train_path = os.path.join(DATA_DIR, 'train.h5')\n",
    "num_train = save_offsets_to_h5(train_data, train_path, max_length)\n",
    "print(f\"Saved {num_train} training examples to {train_path}\")\n",
    "\n",
    "# Process and save validation data\n",
    "val_path = os.path.join(DATA_DIR, 'val.h5')\n",
    "num_val = save_offsets_to_h5(val_data, val_path, max_length)\n",
    "print(f\"Saved {num_val} validation examples to {val_path}\")\n",
    "\n",
    "# Save charset\n",
    "charset_path = os.path.join(DATA_DIR, 'charset.txt')\n",
    "tokenizer.save_charset(charset_path)\n",
    "print(f\"Saved charset to {charset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Prepared Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared datasets\n",
    "with data.H5Dataset(train_path) as train_h5:\n",
    "    mu = train_h5.mu\n",
    "    std = train_h5.std\n",
    "    dataset_max_length = train_h5.max_length\n",
    "\n",
    "print(f\"Dataset statistics:\")\n",
    "print(f\"  Mean (mu): {mu}\")\n",
    "print(f\"  Std: {std}\")\n",
    "print(f\"  Max length: {dataset_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup\n",
    "\n",
    "Set up the model, optimizer, and training components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "alphabet_size = tokenizer.size\n",
    "model = models.SynthesisNetwork.get_default_model(alphabet_size, device)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Alphabet size: {alphabet_size}\")\n",
    "print(f\"  Hidden size: {model.hidden_size}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthesizer for saving/loading\n",
    "mu_tensor = torch.tensor(mu, dtype=torch.float32)\n",
    "sd_tensor = torch.tensor(std, dtype=torch.float32)\n",
    "\n",
    "synthesizer = HandwritingSynthesizer(\n",
    "    model, mu_tensor, sd_tensor, charset, num_steps=dataset_max_length\n",
    ")\n",
    "\n",
    "# Create optimizer\n",
    "# Note: momentum=9000 is used as a flag to enable momentum buffer in CustomRMSprop.\n",
    "# The actual momentum decay is hardcoded to 0.9 in the optimizer implementation.\n",
    "# This follows the original training code from the repository.\n",
    "optimizer = CustomRMSprop(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    alpha=0.95,\n",
    "    eps=1e-4,\n",
    "    momentum=9000,\n",
    "    centered=True\n",
    ")\n",
    "\n",
    "# Gradient clipping values\n",
    "clip_output = config['gradient_clip_output']\n",
    "clip_lstm = config['gradient_clip_lstm']\n",
    "\n",
    "print(\"Optimizer and synthesizer created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop with tqdm and Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader.\"\"\"\n",
    "    x, y = [], []\n",
    "    for points, text in batch:\n",
    "        x.append(points)\n",
    "        y.append(text)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def compute_loss(model, batch, tokenizer, device):\n",
    "    \"\"\"Compute loss for a batch.\"\"\"\n",
    "    points, transcriptions = batch\n",
    "    ground_true = utils.PaddedSequencesBatch(points, device=device)\n",
    "    \n",
    "    batch_size, steps, input_dim = ground_true.tensor.shape\n",
    "    \n",
    "    # Create input (shifted by one step)\n",
    "    prefix = torch.zeros(batch_size, 1, input_dim, device=device)\n",
    "    x = torch.cat([prefix, ground_true.tensor[:, :-1]], dim=1)\n",
    "    \n",
    "    # Create context (one-hot encoded transcriptions)\n",
    "    c = data.transcriptions_to_tensor(tokenizer, transcriptions).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    mixtures, eos_hat = model(x, c)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = losses.nll_loss(mixtures, eos_hat, ground_true)\n",
    "    \n",
    "    return (mixtures, eos_hat), loss\n",
    "\n",
    "\n",
    "def generate_sample(model, tokenizer, mu, std, text, device, max_steps=500):\n",
    "    \"\"\"\n",
    "    Generate a handwriting sample for given text.\n",
    "    \n",
    "    Args:\n",
    "        model: The SynthesisNetwork model\n",
    "        tokenizer: Tokenizer for encoding text\n",
    "        mu: Mean values for denormalization (tuple)\n",
    "        std: Standard deviation values for denormalization (tuple)\n",
    "        text: Text to synthesize\n",
    "        device: PyTorch device\n",
    "        max_steps: Maximum number of points to generate\n",
    "    \n",
    "    Returns:\n",
    "        Tensor on CPU containing denormalized handwriting coordinates\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode text\n",
    "        sentinel = '\\n'\n",
    "        text_with_sentinel = text + sentinel\n",
    "        c = data.transcriptions_to_tensor(tokenizer, [text_with_sentinel]).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        sample = model.sample_means(context=c, steps=max_steps, stochastic=True)\n",
    "        \n",
    "        # Denormalize and move to CPU\n",
    "        sample = sample.cpu() * torch.tensor(std) + torch.tensor(mu)\n",
    "        \n",
    "    model.train()\n",
    "    return sample\n",
    "\n",
    "\n",
    "def visualize_sample(sample, text, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize a handwriting sample and display it.\n",
    "    \n",
    "    Handles cases where visualization fails (e.g., during early training\n",
    "    when the model produces wild coordinates).\n",
    "    \"\"\"\n",
    "    # Use provided save_path or create a default one\n",
    "    if save_path is None:\n",
    "        save_path = os.path.join(SAMPLES_DIR, 'temp_sample.png')\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Check if sample is valid\n",
    "        if sample is None or len(sample) == 0:\n",
    "            print(f\"Text: '{text}' - [Empty sample, model still learning]\")\n",
    "            return\n",
    "        \n",
    "        # Get coordinates to check if they're reasonable\n",
    "        sample_np = sample.cpu().numpy() if hasattr(sample, 'cpu') else sample\n",
    "        x_range = sample_np[:, 0].max() - sample_np[:, 0].min() if len(sample_np) > 0 else 0\n",
    "        y_range = sample_np[:, 1].max() - sample_np[:, 1].min() if len(sample_np) > 0 else 0\n",
    "        \n",
    "        # If coordinates are too wild (happens early in training), show a message\n",
    "        if abs(x_range) > 5000 or abs(y_range) > 5000:\n",
    "            print(f\"Text: '{text}' - [Coordinates too wild: x_range={x_range:.0f}, y_range={y_range:.0f}. Model still learning, this is normal early in training.]\")\n",
    "            return\n",
    "        \n",
    "        # Try to visualize\n",
    "        utils.visualize_strokes(sample, save_path, lines=True, thickness=8)\n",
    "        \n",
    "        # Check if file was created and has content\n",
    "        if os.path.exists(save_path) and os.path.getsize(save_path) > 0:\n",
    "            display(Image(filename=save_path))\n",
    "            print(f\"Text: '{text}'\")\n",
    "        else:\n",
    "            print(f\"Text: '{text}' - [Image not generated - model predictions may be too noisy. This is normal early in training.]\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Text: '{text}' - [Visualization failed: {str(e)[:100]}. Model still learning.]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = config['epochs']\n",
    "BATCH_SIZE = config['batch_size']\n",
    "SAMPLE_INTERVAL = config['sample_interval']  # Show samples every N epochs\n",
    "MODEL_SAVE_DIR = 'colab_checkpoints'\n",
    "SAMPLES_DIR = 'colab_samples'\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Sample every: {SAMPLE_INTERVAL} epochs\")\n",
    "print(f\"  Model save directory: {MODEL_SAVE_DIR}\")\n",
    "print(f\"  Samples directory: {SAMPLES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Initialize best_model_dir before training loop\n",
    "best_model_dir = os.path.join(MODEL_SAVE_DIR, 'best_model')\n",
    "\n",
    "# Open datasets\n",
    "train_dataset = data.NormalizedDataset(train_path, mu, std)\n",
    "val_dataset = data.NormalizedDataset(val_path, mu, std)\n",
    "\n",
    "# Validate datasets are not empty\n",
    "if len(train_dataset) == 0:\n",
    "    train_dataset.close()\n",
    "    val_dataset.close()\n",
    "    raise ValueError(\"Training dataset is empty! Please check your data preparation.\")\n",
    "\n",
    "if len(val_dataset) == 0:\n",
    "    print(\"Warning: Validation dataset is empty. Training will proceed without validation.\")\n",
    "\n",
    "# Get sample texts for visualization\n",
    "sample_texts = []\n",
    "for i in range(min(3, len(val_dataset))):\n",
    "    _, text = val_dataset[i]\n",
    "    sample_texts.append(text)\n",
    "\n",
    "print(f\"Sample texts for visualization: {sample_texts}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "num_train_batches = len(train_loader)\n",
    "num_val_batches = len(val_loader)\n",
    "\n",
    "print(f\"Training batches per epoch: {num_train_batches}\")\n",
    "print(f\"Validation batches per epoch: {num_val_batches}\")\n",
    "\n",
    "# Training\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "try:\n",
    "    epoch_pbar = tqdm(range(1, EPOCHS + 1), desc=\"Training\", unit=\"epoch\")\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        \n",
    "        batch_pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False, unit=\"batch\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(batch_pbar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute loss\n",
    "            y_hat, loss = compute_loss(model, batch, tokenizer, device)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            model.clip_gradients(output_clip_value=clip_output, lstm_clip_value=clip_lstm)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            batch_pbar.set_postfix({'loss': f'{loss.item():.2f}'})\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / num_train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                _, loss = compute_loss(model, batch, tokenizer, device)\n",
    "                epoch_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f'{avg_train_loss:.2f}',\n",
    "            'val_loss': f'{avg_val_loss:.2f}'\n",
    "        })\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            synthesizer.save(best_model_dir)\n",
    "        \n",
    "        # Generate and display samples every SAMPLE_INTERVAL epochs\n",
    "        if epoch % SAMPLE_INTERVAL == 0:\n",
    "            print(f\"\\n\\n{'='*60}\")\n",
    "            print(f\"Epoch {epoch} - Sample Generation\")\n",
    "            print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            for i, text in enumerate(sample_texts[:2]):  # Show 2 samples\n",
    "                sample = generate_sample(model, tokenizer, mu, std, text, device, dataset_max_length)\n",
    "                sample_path = os.path.join(SAMPLES_DIR, f'epoch_{epoch}_sample_{i}.png')\n",
    "                visualize_sample(sample, text, sample_path)\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # Save checkpoint every 20 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            checkpoint_dir = os.path.join(MODEL_SAVE_DIR, f'Epoch_{epoch}')\n",
    "            synthesizer.save(checkpoint_dir)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training Complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "finally:\n",
    "    # Always close datasets to prevent resource leaks\n",
    "    train_dataset.close()\n",
    "    val_dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', color='blue', alpha=0.8)\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (nats)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Smoothed loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "window = min(10, len(train_losses) // 4) if len(train_losses) > 4 else 1\n",
    "if window > 1:\n",
    "    train_smooth = np.convolve(train_losses, np.ones(window)/window, mode='valid')\n",
    "    val_smooth = np.convolve(val_losses, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(train_losses)), train_smooth, label='Training (smoothed)', color='blue')\n",
    "    plt.plot(range(window-1, len(val_losses)), val_smooth, label='Validation (smoothed)', color='orange')\n",
    "else:\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (nats)')\n",
    "plt.title('Smoothed Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Final Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_dir = os.path.join(MODEL_SAVE_DIR, 'best_model')\n",
    "best_synthesizer = HandwritingSynthesizer.load(best_model_dir, device, bias=1.0)\n",
    "\n",
    "print(\"Loaded best model for final samples\\n\")\n",
    "\n",
    "# Generate samples for all validation texts\n",
    "print(\"=\"*60)\n",
    "print(\"Final Generated Samples\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Reload validation data to get texts\n",
    "val_dataset = data.NormalizedDataset(val_path, mu, std)\n",
    "\n",
    "for i in range(min(5, len(val_dataset))):\n",
    "    _, text = val_dataset[i]\n",
    "    \n",
    "    print(f\"\\nSample {i+1}: '{text}'\")\n",
    "    \n",
    "    sample = generate_sample(\n",
    "        best_synthesizer.model, \n",
    "        tokenizer, \n",
    "        mu, std, \n",
    "        text, \n",
    "        device, \n",
    "        dataset_max_length\n",
    "    )\n",
    "    \n",
    "    sample_path = os.path.join(SAMPLES_DIR, f'final_sample_{i}.png')\n",
    "    visualize_sample(sample, text, sample_path)\n",
    "\n",
    "val_dataset.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All samples generated!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = os.path.join(MODEL_SAVE_DIR, 'final_model')\n",
    "synthesizer.save(final_model_dir)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_dir}\")\n",
    "print(f\"Best model saved to: {best_model_dir}\")\n",
    "\n",
    "# List all saved files\n",
    "print(f\"\\nSaved model files:\")\n",
    "for root, dirs, files in os.walk(MODEL_SAVE_DIR):\n",
    "    level = root.replace(MODEL_SAVE_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Download Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the models for download\n",
    "import shutil\n",
    "\n",
    "# Create zip archive\n",
    "shutil.make_archive('handwriting_model', 'zip', MODEL_SAVE_DIR)\n",
    "\n",
    "print(\"Model archive created: handwriting_model.zip\")\n",
    "print(\"\\nTo download, click on the file in the file browser (left sidebar) and download.\")\n",
    "\n",
    "# For Google Colab - download directly\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('handwriting_model.zip')\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab - manual download required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Interactive Synthesis (Optional)\n",
    "\n",
    "Use this cell to generate handwriting for custom text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom text synthesis\n",
    "CUSTOM_TEXT = \"Hallo\"  # Change this to your desired text\n",
    "\n",
    "print(f\"Generating handwriting for: '{CUSTOM_TEXT}'\")\n",
    "\n",
    "# Check if all characters are in charset\n",
    "missing_chars = set(CUSTOM_TEXT) - set(charset)\n",
    "if missing_chars:\n",
    "    print(f\"Warning: Characters not in charset: {missing_chars}\")\n",
    "    print(f\"Available characters: '{charset}'\")\n",
    "else:\n",
    "    # Use best_synthesizer if available, otherwise fall back to model\n",
    "    synthesis_model = best_synthesizer.model if 'best_synthesizer' in dir() and best_synthesizer is not None else model\n",
    "    \n",
    "    sample = generate_sample(\n",
    "        synthesis_model,\n",
    "        tokenizer,\n",
    "        mu, std,\n",
    "        CUSTOM_TEXT,\n",
    "        device,\n",
    "        dataset_max_length\n",
    "    )\n",
    "    \n",
    "    custom_path = os.path.join(SAMPLES_DIR, 'custom_sample.png')\n",
    "    visualize_sample(sample, CUSTOM_TEXT, custom_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Summary\n",
    "\n",
    "This notebook trained a handwriting synthesis model using:\n",
    "\n",
    "- **Dataset**: HuggingFace dataset `finnbusse/v3testing`\n",
    "- **Model**: SynthesisNetwork based on Alex Graves' paper \"Generating Sequences With Recurrent Neural Networks\"\n",
    "- **Architecture**: 3-layer LSTM with soft attention window mechanism\n",
    "\n",
    "### Hyperparameters Used\n",
    "- Batch size: {config['batch_size']}\n",
    "- Epochs: {config['epochs']}\n",
    "- Learning rate: {config['learning_rate']}\n",
    "- Hidden size: {config['hidden_size']}\n",
    "- Gradient clipping: {config['gradient_clip_output']} (output), {config['gradient_clip_lstm']} (LSTM)\n",
    "\n",
    "### Tips for Better Results\n",
    "1. **More data**: Collect more handwriting samples for better generalization\n",
    "2. **Longer training**: Increase epochs if validation loss is still decreasing\n",
    "3. **Data augmentation**: Add slight variations to stroke data\n",
    "4. **Probability bias**: Use bias parameter when generating samples for cleaner output"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
